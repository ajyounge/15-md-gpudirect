%\documentclass[10pt, conference, compsocconf]{IEEEtran}
\documentclass[times,10pt,twocolumn,conference]{IEEEtran}

\usepackage{setspace}
\usepackage{times}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{rotating}
\usepackage[usenames]{color}
\usepackage{setspace}
\usepackage{tabularx,colortbl}
\usepackage{subfigure}
\usepackage{cite}
\usepackage{multicol}
\usepackage[usenames]{color}

\begin{document}

\input{tex/macros}


\title{Supporting High Performance Molecular Dynamics in Virtualized Clusters using IOMMU, SR-IOV, and GPUDirect}
\author{\IEEEauthorblockN{Andrew J. Younge}
\IEEEauthorblockA{School of Informatics \& Computing\\
Indiana University \\
Bloomington, IN 47408 \\
Email: ajyounge@indiana.edu}
\and
\IEEEauthorblockN{John Paul Walters}
\IEEEauthorblockA{Information Sciences Institute\\
University of Southern California\\
Arlington, VA 22203\\
Email: jwalters@isi.edu}
\and
\IEEEauthorblockN{Geoffrey C. Fox}
\IEEEauthorblockA{School of Informatics \& Computing\\
Indiana University \\
Bloomington, IN 47408 \\
Email: gcf@indiana.edu}}


\maketitle

\begin{abstract}

Cloud infrastructure-as-a-Service paradigms have recently shown their utility for a vast array of computational problems, ranging from advanced web service architecutres to high throughput computing.  However, many scientific computing applications have been slow to adapt to virtualized cloud frameworks. This is due to performance impacts of virtualization technologies, coupled with the lack of advanced hardware support necessary for running many high performance scientific appications at scale. 

By using KVM virtual machines that leverage both Nvidia GPUs and InfiniBand, we show that molecular dynamics simulations with LAMMPs and HOOMD run at near-native speeds. This experiment also illustrates how virtualized environments can support the latest parallel computing paradigms, including both MPI+CUDA and new GPUDirect RDMA functionality. Specific findings show initial promise in scaling of such applications to larger production deployments targeting large scale computational workloads.  


%While these experiments do go beyond a single-node, their early implementation is limited to only 4 nodes due to the lack of feasible resources. Currently efforts are under way to scale the deployment to hundreds of cores and 32 GPUs within the FutureGrid testbed, which we look to demonstrate at Supercomputing 2014 in November. 
 

%and Cloud IaaS platforms may be well suited for supporting larger scale scientific applications, including support for the long tail of science. 

%* High performance Cloud IaaS \\
%* Support mid-tier scientific computing \\
%* Long tail of science \\
%* MD simulations \\
%* Good results \\

\end{abstract}

\section{Introduction}

At present we stand at the inevitable intersection between High Performance Computing (HPC) and clouds. Various platform tools such as Hadoop and MapReduce, among others, have already percolated into data intensive computing within HPC \cite{jha2014apache}.  In addition, there are efforts to support traditional HPC-centric scientific computing applications in virtualized cloud infrastructure.  There are a multitude of reasons for supporting parallel computation in the cloud\cite{Armbrust2010}, including features such as dynamic scalability, specialized operating environments, simple management interfaces, fault tolerance, and enhanced quality of service, to name a few. The growing importance of supporting advanced scientific computing using cloud infrastructure can be seen by a variety of new efforts, including the NSF-funded XSEDE Comet resource at SDSC \cite{sdsc2014comet}.  

Nevertheless, there exists a past notion that virtualization used in today's cloud infrastructure is inherently inefficient.  Historically, cloud infrastructure has also done little to provide the necessary advanced hardware capabilities that have become almost mandatory in supercomputers today, most notably advanced GPUs and high-speed, low-latency interconnects.  The result of these notions has hindered the use of virtualized environments for parallel computation, where performance must be paramount.

A growing effort is currently underway that looks to systematically identify and reduce any overhead in virtualization technologies, so far with relative success \cite{Younge2011cloud, Lukoviak}.  Thus, we see a consitantly diminishing overhead with virtualization, not only with traditional overheads \cite{} but also with HPC workloads.  While virtualization will almost always include some additionaly overhead in relation to its dynamic abilities, the eventual goal for supporting HPC in virtualized environments  is to minimize any overhead wheneve rpossible.
 
To advance the placement of HPC applications on virtual machines, new efforts are emerging foscusing specificaly on key HPC hardware. By leveraging new virtualization tools such as IOMMU device passthrough and SR-IOV, we can now support the same common HPC hardware such as the latest Nvidia Tesla GPUs \cite{younge2014}  as well as InfiniBand fabric\cite{panda}.  


Recent advances in hypervisor performance  \cite{Younge2011cloud} coupled with the newfound availably of HPC hardware in virtual machines analogous to the most powerful supercomputers used today, we see can see the formation of a high performance cloud infrastructure. While our previous advances in this area have focused on single-node advancements, it is now imperative to ensure real-world applications can also operate at scale. 

To start, we demonstrate running two molecular dynamics simulations, LAMMPS and HOOMD, in a virtual infrastructure complete with both Kepler GPUs and QDR InfiniBand.  Both LHOOMD and LAMMPS are used extensively in some of the world's fastest supercomputers and represent a key simulation example that HPC supports today.  We show that these applications are able to run at near-native speeds within a completely virtualized environment, demonstrating small performance impacts usually acceptable by many users. Furthermore, we illustrate the ability of such a virtualized environment to support cuttinge edge software tools such as RDMA GPUDirect, demonstrating cutting-edge technologies are indeed possible in a virtualized environment. 

Following these efforts, we hope to ensure upstream infrastructure projects such as OpenStack \cite{www-openstack} are able to make effective and quick use of these features, allowing users to build private cloud infrastructure to support high performance distributed computational workloads. 

%Furthermore, the tighet and exact integration into an open source Cloud infrastructure framework such as OpenStack also becomes a critical next step.  

%This manuscript demonstrates 


%The tight and exact integration into an open source cloud IaaS framework such as OpenStack \cite{www-openstack} becomes critical.

 

%* Broadly talk about clouds, OpenStack, some of our earlier HPC and GPU work\\
%* We've shown single node GPU performance at nearly 100\% efficieny (we'll need to be more accurate/precise than that in the actual submission). \\
%* In this work we demonstrate two molecular dynamics simuations running in a virtual infrastructure: LAMMPS and  HOOMD \\
%* We show that both perform near-native, and we show GPU Direct RDMA for the first time in the cloud \\

\section{Background and Related Work}

NOTE: first introduce virtualization, and I/O featuresets. Then enable GPUs. Then bring in SR-IOv and infiniBand. Finally, discuss applications and GPUDirect.

Virtualization technologies and hypervisors have been seen widespread deployment in support of a vast array of applications.  This ranges from public commercial Cloud deployments such as Amazon EC2 \cite{www-ec2}, Microsoft Azure, and Google's Cloud Platform \cite{www-google-cloud} to private deployments within colocation facilities, corporate data centers, and even national scale cyberinfrastructure initiatives.  All these support look to support various use cases and applications such as web servers, ACID and BASE databases, online object storage, and even distributed systems, to name a few.  

The use of virtualization and hypervisors specifically support various HPC solutions has been studied with mixed results.  In ~\cite{Younge2011cloud}, it is found that there is a great deal of variance between hypervisors when running various distributed memory and MPI applications, finding that KVM overall performed well across an array of HPC benchmarks.  Furthermore, some applications may not may well into default virtualized evnironments, such as High Performance Linpack \cite{piotr-2011-utk}.  

Recently, various CPU architectures have added support for I/O virtualization mechanisms directly in the CPU ISA through the use of an I/O memory management unit (IOMMU). Often, this is referred to as PCI Passthrough, as it enabled devices on the PCI-Express bus to be passed directly to a specific virtual machine (VM).  Specific hardware implementations include Intel's VT-d \cite{intel-vtd}, AMD's IOMMU \cite{amdiommu} from x86\_64 architectures, and even more recently ARM System MMU \cite{arm-systemmmu}.  All of these implementations effectively look to aid in the usage of DMA-capable hardware to be used within a specific virtual machine. Using these features, a wide array of hardware can be utilized directly within VMs and enable fast and efficient computation and I/O capabilities.

With PCI Passthrough, a PCI device is handed directly to a running (or booting) thereby relinquishing complete control of the device within the hostentirely. This is different form typical VM usage where hardware is emulated in the host and used in a guest VM, such as with bridged ethernet adapters or emulated VGA devices. Performing PCI Passthrough requires the host to seize the device upon boot using a specialized driver to effectively block normal driver initialization. In the instance of the KVM hypervisor, this is done using the \em{vfio} and \em{pci\_stub} drivers. Then, this driver relinquishes control to the VM, whereby normal device drivers initiate the hardware and enable the device for use within a given VM.  

\subsection{GPU Passthrough}

Nvidia GPUs comprise the single most common accelerator in the Nov 2014 Top 500 List \cite{top500} and represent an increasing shift towards accelerators for HPC applications. Recently efforts have been seen to support such GPU accelerators directly within VMs using IOMMU technologies, with implementations now available with KVM \cite{walters2014cloud}, Xen \cite{Younge2013hpgc} and VMWare \cite{Vu2014}.  These efforts have shown that GPUs can achieve up to 99\% of their bare metal performance when passed to a virtual machine using PCI passthrough \cite{Walters2014cloud}.  These works demonstrate PCI passthrough performance across a range of hypervisors and GPUs, but were limited to single node performance. 

\subsection{SR-IOV and InfiniBand}

With almost all parallel HPC applications, the interconnect fabric necessary to enable efficient communication between processors becomes a central requirement to acheiving good performance. Specifically, a high bandwidth link is needed for distruted processors to share large amounts of data. Futhermore, low latency becomes equally important for speeding small message communications and resolving large barriers within parallelized code. One such interconnect, InfiniBand, has become the most common implementation used within the Top500 list. Previously, InfiniBand was inaccessable to virtualized environments.  

Supporting I/O interconnects in VMs has been aided by Single Root I/O Virtualization (SR-IOV), whereby multiple virtual PCI functions are created in hardware to represent a single PCI device. These virtual functions (VFs) can then be passed to a VM and used as if it had direct access to that PCI device. SR-IOV allows for the virtualization and multiplexing to be done within the hardware, allowing for higher performance and greater control. 

SR-IOV has been used in conjunction with Ethernet etensively to provide high performance 10Gb TCP/IP connectivity within VMs \cite{noidea}, offering near-native bandwidth and advanced QoS features not easily obtained through emulated Ethernet offerings. Currently Amazon EC2 offers a high performance VM solution utilizing SR-IOV enabled 10Gb Ethernet adapters. However, Ethernet dos not offer the high bandwidth or low latency typically found with InfiniBand. 

Recently SR-IOV support for InfiniBand has been added by Mellanox in the ConnectX series adapters. Initial evaluation of SR-IOV InfiniBand within KVM VMs has proven has found point-to-point bandwidth to be near-native, but up to 30\% latency overhead  latency for small messages \cite{SRIOVInfiniBand, panda, fermiIB}. However, even with the noted overhead, this still signifies up to an order of magnitude difference in latency between InfiniBand and Ethernet with VMs. Furthermore, advanced configuration of SR-IOV enabled Infiniband fabric has taken shape, with recent research showing up to a 30\% reduction latency \cite{musleh2014}. However, real application performance has not yet been well understood. 


\input{benchmarks}




%Similarly, Infiniband SR-IOV (Single Root I/O Virtualization) has been evaluated within the context of microbenchmarks~\cite{SRIOVInfiniband,Musleh2014cloud}, but performance for real applications is not yet well-understood.

%moved to related work. right placement?
%Recent work recent work has focused on single-node performance.  In \cite{walters2014}, we've shown how the latest Kepler GPUs from Nvidia  with Sandy-Bridge Intel Xeon CPUs can perform at near-native performance running various workloads across wide range of hypervisors. Furthermore, advanced configuration of SR-IOV enabled Infiniband fabric has taken shape, with recent research showing up to a 30\% reduction latency \cite{musleh2014}.  



\section{Experimental Setup}

Using two molecular dynamics tools, LAMMPS\cite{plimpton2007lammps} and HOOMD~\cite{anderson2010hoomd}, we demonstrate a high performance \textit{system}.  That is, we combine PCI passthrough for Nvidia Kepler-class GPUs with QDR Infiniband SR-IOV and show that high performance molecular dynamics simulations are achievable within a virtualized environment. 

For the first time, we also demonstrate Nvidia GPUDirect technology within such a virtual environment.  Thus, we look to not only illustrate that virtual machines provide a flexible high performance infrastructure for scaling scientific workloads including MD simulations, but also that the latest HPC features and programming environments are also available in this same model.   

\subsection{Node configuration}

To support the use of Nvidia GPUs and InfiniBand within a VM, specific and exact configuration is needed. This node configuration is illustrated in figure \ref{F:passthrough}, and while our implementation is specific to KVM, this setup represents a design that can be hypervisor agnostic.

\FIGURE{!htb}
  {images/host-pci-passthrough.png}
  {1.0}
  {LAMMPS RHODO \& LJ Performance}
  {F:passthrough}


Each node in the testbed uses CentOS 6.4 with a 3.13 upstream Linux kernel was used as the host OS, along with the KVM hypervisor and the vfio driver. 
Each Guest VM runs Centos 6.4 with a stock 2.6.32-358.23.2 kernel.
A Kepler GPU is passed through using PCI Passthrough and directly initiated within the VM via the Nvidia 331.20 driver and CUDA release 5.5. While this specific implementation used only a single GPU, it is also possible to include as many GPUs as one can fit within the PCI Express bus if desired. As the GPU is used by the VM, an onboard VGA device was used by the host and a standard Cirris VGA was emulated in the guest OS. 

With using SR-IOV, the OFED drivers version 2.1-1.0.0 are used with Mellanox ConnectX-3 VPI adapter with firmware ???.  The host driver initiates 4 VFs, one of which is passed through to the VM where the default OFED mlnx\_ib drivers are loaded.  

%The native bare-metal base system and all guest VMs are composed of a CentOS 6.4 installation with a 2.6.32-358.23.2 stock kernel, MVAPICH 2.0 GDR, and CUDA version 5.5. Each guest was allocated 20 GB of RAM and a full socket (8 cores) as well as a single InfiniBand virtual function  and 1 Kepler GPU per VM.  


%CentOS 6.4 with a 3.13 upstream Linux kernel was used as the host OS with the KVM hypervisor.  The native bare-metal base system and all guest VMs are composed of a CentOS 6.4 installation with a 2.6.32-358.23.2 stock kernel, MVAPICH 2.0 GDR, and CUDA version 5.5. Each guest was allocated 20 GB of RAM and a full socket (8 cores) as well as a single InfiniBand virtual function  and 1 Kepler GPU per VM.  

\subsection{Cluster Configuration}

Our test environment is composed of 4 servers each with a single Nvidia Kepler-class GPU.  Two servers are equipped with K20 GPUs, while the other two servers are equipped with K40 GPUs, demonstrating the potential for a more heterogenious deployment.  Each server is composed of 2 Intel Xeon E5-2670 CPUs, 48GB of DDR3 memory, and Mellanox ConnectX-3 QDR Infiniband.  CPU sockets and memory are split evenly between the two NUMA nodes on each system. All InfiniBand adapters use a single Mellanox SwitchX QDR switch running an updated subnet manager for IPoIB functionality.   


For these experiments, both the GPUs and Infiniband adapters are attached to NUMA node 1 and both the guest VMs and the base system utilized identical software stacks.  Each guest was allocated 20 GB of RAM and a full socket of 8 cores, and pinned to NUMA node 1 to ensure optimal hardware usage. While all VMs are capable of login via the InfiniBand IPoIB setup, a 1Gb Ethernet network was used for all management and login tasks.  

%Our test environment is composed of 4 servers each with a single Nvidia Kepler-class GPU.  Two servers are equipped with K20 GPUs, while the other two servers are equipped with K40 GPUs, demonstrating the potenti

For a fair and effective comparison, we also use a native environment without any virtualization. This native environment employs the same hardware configuration, and like the Guest OS runs CentOS 6.4 with the stock 2.6.32-358.23.2 kernel. 

%In order to effectively test MD simulations in LAMMPS and HOOMD beyond single-node tests, ronment.  Bespin includes 4 blades, each with 2 Intel Xeron E5-2670 CPUs, 48Gb DDR3 memory, Mellanox ConnectX3 QDR InfiniBand cards, and a mixture of Nvidia Kepler series K20 and K40 GPUs.  While the effective experimental hardware allocation remains relatively low compared to production runs of either application, it does allow for a useful evaluation at a larger scale than preivously evaluated as well as a valued extrapolation to larger resources.

 
\section{Results}

In this section, we discuss the performance of both the LAMMPS and HOOMD molecular dynamics simulation tools when running within a virtualized environment. Specifically, we scale each application to 32 cores and 4 GPUs, both in native and virtualized environments.  Each application set was run 10 times, with the results averaged accordingly. 

\subsection{LAAMPS}


Figure~\ref{F:lammps-lj} shows one of the most common LAMMPS algorithms used; the Lennard-Jones potential (LJ).  This algorithm is deployed in two main configurations - a 1:1 core to GPU mapping, or a 8:1 core to GPU mapping.  With small problem sizes, the 1:1 mapping outperforms the more complex core deployment, as the problem does not require the additional complexity of SIMD CPU computation.  However, as expected the multi-core configuration quickly outperforms the former for higher problem sizes, achieving roughly twice the performance with all 8 available cores keeping the GPU busy with work. 

Performance of the virtualized environment performs very well compared to the best-case native deployment. For the multi-core configuration across all problem sizes, the virtualized deployment averaged 98.5\% efficiency compared to native. The single core per GPU deployment reported better-than native performance at 100\% native. 

 %and the Rhodopsin protein in solvated lipid bilayer benchmark (Rhodo), both running with the GPU package across 8 cores per GPU. Here we see that both benchmarks scale remarkably well in the virtualized KVM guest environment. 
%Compared to the base system performance, the VMs running LAMMPs acheive 96.7\% and 99.3\% efficiency for LJ and Rhodo, respectively when running across all nodes.  These low overheads illustrate the utility of running LAMPS on cloud infrastructure, and also hold promise for other hybrid MPI + CUDA applications to also scale well in a virtualized environment. 

\FIGURE{!htb}
  {images/lammps-lj-scale.png}
  {1.0}
  {LAMMPS LJ Performance}
  {F:lammps-lj}


Another common LAMMPs algoirthms, the Rhodopsin protein in solvated lipid bilayer benchmark (Rhodo) was also run with results given in Figure \ref{F:lammps-rhodo}. As with the LJ runs, we see the multi-core to GPU configuration resulting in higher computational performance for the larger problem sizes compared to the single core per GPU configuration.  

\FIGURE{!htb}
  {images/lammps-rhodo-scale.png}
  {1.0}
  {LAMMPS RHODO Performance}
  {F:lammps-rhodo}

Again, the overhead of the virtualized configuration remains low across all configurations and problem sizes, with an average 96.4\% effiency compared to native. Interestingly enough, we also see the performance gap decrease as the problem size increases, with the 512k problem size in yielding 99.3\% of native performance.




\subsection{HOOMD}




% Compared to the base system's performance, we see overheads of 3.2\% and 0.6\% for the LJ and Rhodo benchmarks, respectively, when running 8 cores per GPU at experimental scale. 


In Figure~\ref{F:HOOMD} we show the performance of a Lennard-Jones liquid
simulation with 256K particles running under HOOMD.  HOOMD includes support for CUDA-aware MPI implementations via GPUDirect.  The MVAPICH 2.0 GDR implementation enables a further optimization by supporting RDMA for GPUDirect. From Figure~\ref{F:HOOMD} we can see that HOOMD simulations, both with and without GPUDirect, perform very near-native.  The GPUDirect results at 4 nodes (32 cores) achieve 98.5\% of the base system's performance.  The non-GPUDirect results achieve 98.4\% efficiency at 4 nodes. These results indicate the virtualized HPC environment is able to support such complex workloads. While the effective testbed size is relatively small, it indicates that such workloads may scale equally well to hundreds or thousands of nodes. 

\FIGURE{!htb}
  {images/hoomd.png}
  {1.0}
  {HOOMD LJ Performance with 256k Simulation}
  {F:HOOMD}


\section{Discussion}

From the results, we see the potential for running HPC applications in a virtualized environment using GPUs and InfiniBand interconnect fabric. Across all LAAMPS runs with raning core configurations, we found only a 1.9\% overhead between the KVM virtualized environment and native. For HOOMD, we found a similar 1.5\% overhead, both with and without GPU Direct. These results go against conventional wisdom that HPC workloads do not work in VMs. In fact ,we show two N-Body type simulations programmed in an MPI+CUDA implementation perform at roughly near-native performance in tuned KVM virtual machines.  

With HOOMD, we see how GPUDirect RDMA shows a clear advantage over the non-GPUDirect implementation, achieving a 9\% performance boost in both the native a virtualized experiments.  While GPU Direct's performance impact has been well evaluated preivously \cite{GPUDirect}, it is the author's belief that this manuscript represents the first time GPUDirect has has been utilized in a virtualized environment.  

Another interesting finding of running LAMMPS and HOOMD in a virtualized environment is as workload scales from a single node to 32 cores, we find the overhead does not increase. this lends credence to the idea that such a solution would also work for a much larger deployment. Specifically, it would be possible to ramp up a similar deployment in FutureGrid \cite{www-futuregrid} or even the planned NSF Comet machine at SDSC, scheduled to provide up to 2 Petaflops of computational power. Effectively, these results support the theory that a majority of HPC computations can be supported in virtualized environment. 

TODO: Discuss integration into OpenStack via PCI-Passthrough of GPUs (done) as well as InfiniBand integration (somewhat done, working on it for SR-IOV), Mellanox Neutron plugin. Describe possibility of a virtual cluster (keyword).  

\section{Conclusion}

With the advent of cloud infrastructure, the ability to run large-scale parallel scientific applications has become possible but limited due to both performance and hardware availability issues. In this work we show that advanced HPC-oriented hardware such as the latest Nvidia GPUs and InfiniBand fabric are now available within a virtualized infrastructure. Our results find MPI + CUDA applications run at near-native performance compared to traditional non-virtualized HPC infrastructure, with just an averaged 1.9\% and 1.5\% overhead for LAMMPs and HOOMD, respectively. Moving forward, we show the utility of GPUDirect RDMA for the first time in a cloud environment with HOOMD.  Effectively, we look to pave the way for large-scale virtualized cloud Infrastructure to support a wide array of advanced scientific computation commonly found running on many supercomputers today.  Efforts are also underway to leverage these technologies and provide them in an open source Infrastructure-as-a-Service framework such as OpenStack.  



%The tight and exact integration into an open source cloud IaaS framework such as OpenStack \cite{www-openstack} becomes critical.

%* Multi-node virtualized MD at low overhead \\

%* First to show GDR in virtual machine \\

\bibliographystyle{tex/IEEEtran}


\bibliography{biblio}

\end{document}
