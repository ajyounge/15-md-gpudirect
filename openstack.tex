\section{A Cloud for High Performance Computing}\label{openstack}
With support for GPU Passthrough, SR-IOV, and GPUDirect, we have the building
blocks for a high performance, heterogeneous cloud.  In addition, other common
accelerators (e.g. Xeon Phi~\cite{Phi}) have similarly been demonstrated in
virtualized environments.  We envision a heterogeneous cloud that supports
both high speed networking and accelerators for tightly coupled applications.

To this end we developed a heterogeneous cloud based on
OpenStack~\cite{www-openstack}.  In our previous work, we demonstrated the ability to rapidly provision GPU, bare metal, and other
heterogeneous resources within a single cloud~\cite{crago2011heterogeneous}.
Building on this effort we have added support for GPU passthrough to OpenStack
as well as prototyped SR-IOV support for ConnectX-2 and ConnectX-3 Infiniband devices.
Mellanox  has since added an OpenStack InfiniBand networking plugin for
OpenStack's Neutron service~\cite{ML2}. %Our institutional requirements depend on ConnecteX-2
%SR-IOV support, requiring an independent implementation. 
While OpenStack supports services for networking (Neutron), compute (Nova), identity
(Keystone), storage (Cinder, Swift), and others, our work focuses entirely
on the compute service.  

Scheduling is implemented at two levels: the cloud-level and the node-level.  In
our earlier work, we have developed a cloud-level heterogeneous scheduler for OpenStack that 
allows scheduling based on architectures and
resources~\cite{crago2011heterogeneous}.  In this model, the cloud-level
scheduler dispatches jobs to nodes based on resource requirements (e.g. Kepler
GPU) and node-level resource availability.

At the node, a second level of scheduling occurs to ensure that resources are
tracked and not over-committed.  Unlike traditional cloud paradigms, devices
passed into VMs cannot be over-committed.  We treat devices, whether GPUs or
InfiniBand virtual functions, as schedulable resources.  Thus, it is the responsibility of the
individual node to track resources committed and report availability to the
cloud-level scheduler.  For reporting, we augment OpenStack's
existing reporting mechanism to provide a low overhead solution.


